{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samochristian2020/Data-Science-and-Statistics/blob/main/Deep_learning/Fine_Tuning_Mobilenetv2_Classifier_Oxford_102.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emu8B7r8MN76"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y00b5TQZnqs_"
      },
      "source": [
        "# AI application\n",
        "\n",
        "Going forward, AI algorithms will be incorporated into more and more everyday applications. For example, we might want to include an image classifier in a smart phone app. To do this, we'd use a deep learning model trained on hundreds of thousands of images as part of the overall application architecture.\n",
        "In this project, we'll train an image classifier to recognize different species of flowers. We'll be using [this dataset](http://www.robots.ox.ac.uk/~vgg/data/flowers/102/index.html) from Oxford of 102 flower categories, we can see a few examples below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjxUmQsoWcXb"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "im = Image.open('/content/drive/MyDrive/test_images/assets/Flowers.png')\n",
        "im"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "of6NKQ92hgne"
      },
      "source": [
        "The project is broken down into multiple steps:\n",
        "\n",
        "* Load the image dataset and create a pipeline.\n",
        "* Build and Train an image classifier on this dataset.\n",
        "* Use our trained model to perform inference on flower images.\n",
        "\n",
        "We'll implement this project in Python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKnPjnLAftRV"
      },
      "source": [
        "## Import Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-Idj2t4h7kp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KERAS_BACKEND'] = 'tensorflow'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HTtns5vnBWL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Agcmuzwnb0r"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "print(tfds.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIY2QGR4oqBn"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dCk6873paNW"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "logger = tf.get_logger()\n",
        "logger.setLevel(logging.ERROR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWKF0YOarpCx"
      },
      "source": [
        "## Load the Dataset\n",
        "\n",
        "Here we'll use `tensorflow_datasets` to load the [Oxford Flowers 102 dataset](https://www.tensorflow.org/datasets/catalog/oxford_flowers102). This dataset has 3 splits: `'train'`, `'test'`, and `'validation'`.  we'll also need to make sure the training data is normalized and resized to 224x224 pixels as required by the pre-trained networks.\n",
        "\n",
        "The validation and testing sets are used to measure the model's performance on data it hasn't seen yet, but we'll still need to normalize and resize the images to the appropriate size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXISRjfdrrQ6"
      },
      "outputs": [],
      "source": [
        "# #: Load the dataset with TensorFlow Datasets.\n",
        "dataset, dataset_info = tfds.load('oxford_flowers102',\n",
        "                                  split=['train[:80%]','test[:5%]','test[5%:10%]'],\n",
        "                                  with_info=True,\n",
        "                                  as_supervised=True)\n",
        "\n",
        "# #: Create a training set, a validation set and a test set.\n",
        "training_set, validation_set, test_set = dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1OiADIaWjBY"
      },
      "outputs": [],
      "source": [
        "print(dataset_info.description)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5pdQnDbf0-j"
      },
      "source": [
        "## Explore the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XikJ4X7FUv8v"
      },
      "outputs": [],
      "source": [
        "# #: Get the number of examples in each set from the dataset info.\n",
        "\n",
        "print('training_set contains {} examples'.format(training_set.cardinality()))\n",
        "print('validation_set contains {} examples'.format(validation_set.cardinality()))\n",
        "print('test_set contains {} examples'.format(test_set.cardinality()))\n",
        "\n",
        "\n",
        "\n",
        "# #: Get the number of classes in the dataset from the dataset info.\n",
        "print('\\n the number of classes in this dataset is :{}'.format(dataset_info.features['label'].num_classes))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWR9ScCbPI_D"
      },
      "outputs": [],
      "source": [
        "# #: Print the shape and corresponding label of 3 images in the training set.\n",
        "size_train = training_set.cardinality()\n",
        "nb = np.random.choice(size_train, 3)\n",
        "nb\n",
        "for i, j in enumerate(training_set):\n",
        "  if i in nb:\n",
        "    print('shape of image of index :{}, is :{}'.format(i,j[0].shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f0R-eiSfc90"
      },
      "outputs": [],
      "source": [
        "def plot_3images (idx, set):\n",
        "\n",
        "  f= plt.figure(figsize=(12, 12))\n",
        "  ax = f.add_subplot(1, 3, 1), f.add_subplot(1, 3, 2), f.add_subplot(1, 3, 3)\n",
        "\n",
        "  j=0\n",
        "  if j<3:\n",
        "    for i, datapoint in enumerate(set):\n",
        "      if i in idx:\n",
        "        image, label= datapoint\n",
        "        image = image.numpy().squeeze()\n",
        "        label = label.numpy()\n",
        "        # Plot the image\n",
        "\n",
        "        ax[j].imshow(image)\n",
        "\n",
        "        ax[j].set_title('The label of this image is: {}'.format(label))\n",
        "        j =j + 1\n",
        "  return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vd-mvbVh7Gf"
      },
      "outputs": [],
      "source": [
        "plot_3images(nb, training_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXlJcLR9hiLK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ7hdD2SXKrF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuh1841cs-j1"
      },
      "source": [
        "### Label Mapping\n",
        "\n",
        "we'll also need to load in a mapping from label to category name. we can find this in the file `label_map.json`. It's a JSON object which we can read in with the [`json` module](https://docs.python.org/3.7/library/json.html). This will give we a dictionary mapping the integer coded labels to the actual names of the flowers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoVzdO3KsdSk"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open('/content/drive/MyDrive/label_map.json', 'r') as f:\n",
        "    class_names = json.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLL4a4Zytbtw"
      },
      "outputs": [],
      "source": [
        "def plot_3images_names (idx, set):\n",
        "\n",
        "  f= plt.figure(figsize=(12, 12))\n",
        "  ax = f.add_subplot(1, 3, 1), f.add_subplot(1, 3, 2), f.add_subplot(1, 3, 3)\n",
        "\n",
        "  j=0\n",
        "  if j<3:\n",
        "    for i, datapoint in enumerate(set):\n",
        "      if i in idx:\n",
        "        image, label= datapoint\n",
        "        image = image.numpy().squeeze()\n",
        "        label = label.numpy()\n",
        "        # Plot the image\n",
        "\n",
        "        ax[j].imshow(image)\n",
        "\n",
        "        ax[j].set_title('The label of this image is: {}'.format(class_names[str(label)]))\n",
        "        j =j + 1\n",
        "  return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK0OM3SgiFtB"
      },
      "outputs": [],
      "source": [
        "plot_3images_names(nb, training_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gL7AaqNf-NC"
      },
      "source": [
        "## Create Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok2N-ykMdSPS"
      },
      "source": [
        "we'll also need to make sure the training, validation and test data are normalized and resized to 224x224 pixels as required by the pre-trained networks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ziG-zJ2hl7DD"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "keras.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DM3SCKy1mGCA"
      },
      "outputs": [],
      "source": [
        "from keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input #, decode_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KpPu6EKNsc7Q"
      },
      "outputs": [],
      "source": [
        "from keras import ops\n",
        "one_hot = keras.layers.CategoryEncoding(num_tokens=102,\n",
        "                                        output_mode='one_hot')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgCIbrSBnLe1"
      },
      "outputs": [],
      "source": [
        "def normalize_im(image, label):\n",
        "\n",
        "  image = ops.image.resize(image, (224, 224))\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  x = preprocess_input(image)\n",
        "  label = one_hot(label)\n",
        "  label = tf.reshape(label, [-1])\n",
        "\n",
        "  return x, label\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTVYjJDAe4tH"
      },
      "outputs": [],
      "source": [
        "from tensorflow import data as tf_data\n",
        "B=64\n",
        "nb_train = size_train\n",
        "train_batches = training_set.cache().shuffle(nb_train//4).map(normalize_im).batch(B).prefetch(tf_data.AUTOTUNE)\n",
        "\n",
        "validation_batches = validation_set.cache().map(normalize_im).batch(B).prefetch(tf_data.AUTOTUNE)\n",
        "\n",
        "test_batches = test_set.cache().map(normalize_im).batch(B).prefetch(tf_data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m34Fjjimspez"
      },
      "outputs": [],
      "source": [
        "list(train_batches.take(5))[3][1][10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gR9gtRbeXPYx"
      },
      "source": [
        "# Build and Train the Classifier\n",
        "\n",
        "Now the data is ready, let's build and train the classifier. we should use the MobileNet pre-trained model from TensorFlow Hub to get the image features. Build and train a new feed-forward classifier using those features.\n",
        "\n",
        "Things we'll do:\n",
        "\n",
        "* Load the MobileNet pre-trained network from TensorFlow Hub.\n",
        "* Define a new, untrained feed-forward network as a classifier.\n",
        "* Train the classifier.\n",
        "* Plot the loss and accuracy values achieved during training for the training and validation set.\n",
        "* Save our trained model as a Keras model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yA2rgMzvl2S"
      },
      "outputs": [],
      "source": [
        "base_model = MobileNetV2(input_shape=(224,224,3),\n",
        "                         alpha=1.0,\n",
        "                         include_top=False,\n",
        "                         weights=\"imagenet\",\n",
        "                         input_tensor=None,\n",
        "                         pooling=None,\n",
        "                         classes=102,\n",
        "                         classifier_activation=\"softmax\",\n",
        "                         name=None,)\n",
        "base_model.trainable = False\n",
        "in_data = keras.Input(shape=(224, 224, 3))\n",
        "inputs = base_model(in_data, training=False)\n",
        "inputs = keras.layers.GlobalAveragePooling2D()(inputs)\n",
        "inputs = keras.layers.Dropout(0.2)(inputs)\n",
        "\n",
        "outputs = keras.layers.Dense(102, activation='softmax')(inputs)\n",
        "modell =keras.Model(in_data, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REU4A8zLbuu5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zirzixi5z2PX"
      },
      "outputs": [],
      "source": [
        "modell.summary(show_trainable=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8nB8mJi1uWZ"
      },
      "outputs": [],
      "source": [
        "modell.compile(optimizer = keras.optimizers.Adam(),\n",
        "               loss= keras.losses.CategoricalCrossentropy(),\n",
        "               metrics=[keras.metrics.CategoricalAccuracy()])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Top Layer\n"
      ],
      "metadata": {
        "id": "wL2MVIZWLBPp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIDKg9683Elh"
      },
      "outputs": [],
      "source": [
        "#fitting the top layer of our model\n",
        "\n",
        "history = modell.fit(train_batches,\n",
        "                     epochs=100,\n",
        "                     validation_data= validation_batches,\n",
        "                     callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdP5QLB2WAoB"
      },
      "outputs": [],
      "source": [
        "history.history.keys()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zqV896Wk_yGk"
      },
      "outputs": [],
      "source": [
        "acc = history.history['categorical_accuracy']\n",
        "val_acc = history.history['val_categorical_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "#plt.ylim([0,1.3])\n",
        "plt.ylim([min(plt.ylim()),1.3])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0, max(plt.ylim())+0.31])\n",
        "#plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine Tuning the whole Network"
      ],
      "metadata": {
        "id": "CRQK_IT9Ktrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = True\n",
        "modell.summary(show_trainable=True)\n"
      ],
      "metadata": {
        "id": "SCE-evERK7Cy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modell.compile(optimizer = keras.optimizers.Adam(learning_rate=0.0001),\n",
        "               loss= keras.losses.CategoricalCrossentropy(),\n",
        "               metrics=[keras.metrics.CategoricalAccuracy()])"
      ],
      "metadata": {
        "id": "TqQkka7kLbpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history2 = modell.fit(train_batches,\n",
        "                     epochs=100,\n",
        "                     validation_data= validation_batches,)\n",
        "                     #callbacks=[keras.callbacks.EarlyStopping(patience=2)])"
      ],
      "metadata": {
        "id": "8Zb40BfXT_9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = modell.evaluate(test_batches)\n",
        "print('test loss, test acc:', results)"
      ],
      "metadata": {
        "id": "VYMvmn0rjPQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc2 = history2.history['categorical_accuracy']\n",
        "val_acc2 = history.history['val_categorical_accuracy']\n",
        "\n",
        "loss2 = history2.history['loss']\n",
        "val_loss2 = history2.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc2, label='Training Accuracy')\n",
        "plt.plot(val_acc2, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "#plt.ylim([0,1.3])\n",
        "plt.ylim([min(plt.ylim()),1.3])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss2, label='Training Loss')\n",
        "plt.plot(val_loss2, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([-0.3, max(plt.ylim())+0.31])\n",
        "#plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WDjFmNm-URk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcTDnyvop3ky"
      },
      "source": [
        "## Testing our Network\n",
        "\n",
        "It's good practice to test our trained network on test data, images the network has never seen either in training or validation. This will give we a good estimate for the model's performance on completely new images. we should be able to reach around 70% accuracy on the test set if the model has been trained well."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modell.evaluate(test_batches)"
      ],
      "metadata": {
        "id": "q_AOjZETIda9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79l7-HM1cafO"
      },
      "outputs": [],
      "source": [
        "# #: Print the loss and accuracy values achieved on the entire test set.\n",
        "\n",
        "results = modell.evaluate(test_batches)\n",
        "print('test loss, test acc:', results)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**:  We can see a slight improvement in the accuracy of our model: 77 % before and 84% after fine tuning, on the test set.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rV5VH4Oqp5Wl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5GXSnXd3W5DE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLsIDWnuqfkl"
      },
      "source": [
        "## Save the Model\n",
        "\n",
        "Now that our network is trained, let's save it so we can load it later for making inference. In the cell below save our model as a Keras model (*i.e.* save it as an HDF5 file)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XOwdOjSptp-"
      },
      "outputs": [],
      "source": [
        "# #: Save our trained model as a Keras\n",
        "\n",
        "#path_name = \"./flower_image_classifier.h5\"\n",
        "#modell.save(path_name)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}